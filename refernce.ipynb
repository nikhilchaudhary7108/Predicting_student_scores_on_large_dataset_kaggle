{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9cf55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "STUDENT TEST SCORE PREDICTION - COMPREHENSIVE TUTORIAL\n",
    "================================================================================\n",
    "\n",
    "This tutorial will guide you through building a machine learning model to predict\n",
    "student test scores. We'll go step-by-step with theory and code together.\n",
    "\n",
    "Author: Nikhil Chaudhary\n",
    "Purpose: Educational - Learn ML by doing\n",
    "Competition: Kaggle Playground Series S6E1\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: SETUP AND IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "THEORY - Why do we need these libraries?\n",
    "=========================================\n",
    "\n",
    "1. pandas (pd): For working with tabular data (like Excel spreadsheets)\n",
    "   - Think of it as Excel in Python\n",
    "   - We can filter, sort, calculate, and manipulate data easily\n",
    "\n",
    "2. numpy (np): For numerical operations and arrays\n",
    "   - Fast mathematical computations\n",
    "   - Works well with pandas for calculations\n",
    "\n",
    "3. matplotlib & seaborn: For creating visualizations\n",
    "   - matplotlib: Low-level plotting (more control)\n",
    "   - seaborn: High-level plotting (prettier, easier)\n",
    "\n",
    "4. sklearn (scikit-learn): The main machine learning library\n",
    "   - Contains algorithms, preprocessing tools, and evaluation metrics\n",
    "   \n",
    "5. warnings: To suppress unnecessary warning messages\n",
    "\"\"\"\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')  # Makes plots look nice\n",
    "sns.set_palette(\"husl\")  # Color palette for plots\n",
    "\n",
    "# Display settings for pandas\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.max_rows', 100)      # Show up to 100 rows\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)  # 2 decimal places\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: LOADING AND UNDERSTANDING THE DATA\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "THEORY - What is our data?\n",
    "===========================\n",
    "\n",
    "In machine learning, we work with:\n",
    "1. TRAINING DATA: Data we use to teach our model (has both features and target)\n",
    "2. TESTING DATA: Data we use to make predictions (has features, no target)\n",
    "\n",
    "FEATURES (X): The input variables we use to make predictions\n",
    "   Examples: study hours, attendance, previous scores, etc.\n",
    "\n",
    "TARGET (y): What we want to predict\n",
    "   In this case: student test scores\n",
    "\n",
    "The goal: Learn patterns from training data to predict scores for test data\n",
    "\"\"\"\n",
    "\n",
    "# Load the datasets\n",
    "print(\"\\nðŸ“‚ LOADING DATA...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Replace these paths with your actual file paths\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "print(f\"âœ“ Training data loaded: {train_df.shape[0]} rows, {train_df.shape[1]} columns\")\n",
    "print(f\"âœ“ Testing data loaded: {test_df.shape[0]} rows, {test_df.shape[1]} columns\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: INITIAL DATA EXPLORATION (First Look)\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "THEORY - Why explore data first?\n",
    "==================================\n",
    "\n",
    "Before building any model, we must understand our data:\n",
    "1. What columns do we have?\n",
    "2. What data types (numbers, text)?\n",
    "3. Are there missing values?\n",
    "4. What's the range of values?\n",
    "5. Are there any obvious patterns or issues?\n",
    "\n",
    "This is like getting to know a new friend before working together!\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nðŸ” INITIAL DATA EXPLORATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. First few rows\n",
    "print(\"\\n1. FIRST 5 ROWS OF TRAINING DATA:\")\n",
    "print(\"-\" * 80)\n",
    "print(train_df.head())\n",
    "\n",
    "# 2. Dataset information\n",
    "print(\"\\n2. DATASET INFORMATION:\")\n",
    "print(\"-\" * 80)\n",
    "print(train_df.info())\n",
    "\n",
    "# 3. Basic statistics\n",
    "print(\"\\n3. STATISTICAL SUMMARY:\")\n",
    "print(\"-\" * 80)\n",
    "print(train_df.describe())\n",
    "\n",
    "# 4. Check for missing values\n",
    "print(\"\\n4. MISSING VALUES CHECK:\")\n",
    "print(\"-\" * 80)\n",
    "missing_train = train_df.isnull().sum()\n",
    "missing_test = test_df.isnull().sum()\n",
    "\n",
    "print(\"Training Data Missing Values:\")\n",
    "print(missing_train[missing_train > 0] if missing_train.sum() > 0 else \"No missing values!\")\n",
    "\n",
    "print(\"\\nTesting Data Missing Values:\")\n",
    "print(missing_test[missing_test > 0] if missing_test.sum() > 0 else \"No missing values!\")\n",
    "\n",
    "# 5. Data types breakdown\n",
    "print(\"\\n5. DATA TYPES BREAKDOWN:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Numerical columns: {train_df.select_dtypes(include=[np.number]).columns.tolist()}\")\n",
    "print(f\"Categorical columns: {train_df.select_dtypes(include=['object']).columns.tolist()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 4: DETAILED DATA QUALITY REPORT\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "THEORY - What is a Data Quality Report?\n",
    "========================================\n",
    "\n",
    "A Data Quality Report helps us understand:\n",
    "1. COMPLETENESS: How much data is missing?\n",
    "2. UNIQUENESS: How many unique values per column?\n",
    "3. VALIDITY: Are values in expected ranges?\n",
    "4. CONSISTENCY: Do values make sense?\n",
    "\n",
    "Think of it as a health check-up for your data!\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nðŸ“Š DATA QUALITY REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def create_data_quality_report(df, df_name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Creates a comprehensive data quality report for a dataframe\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The dataframe to analyze\n",
    "    df_name : str\n",
    "        Name of the dataset (for display)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame with quality metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    quality_report = pd.DataFrame({\n",
    "        'Column': df.columns,\n",
    "        'Data_Type': df.dtypes.values,\n",
    "        'Missing_Count': df.isnull().sum().values,\n",
    "        'Missing_Percentage': (df.isnull().sum() / len(df) * 100).values,\n",
    "        'Unique_Count': df.nunique().values,\n",
    "        'Unique_Percentage': (df.nunique() / len(df) * 100).values,\n",
    "        'Sample_Values': [df[col].dropna().head(3).tolist() for col in df.columns]\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{df_name} - QUALITY REPORT:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(quality_report.to_string(index=False))\n",
    "    print(f\"\\nTotal Rows: {len(df)}\")\n",
    "    print(f\"Total Columns: {len(df.columns)}\")\n",
    "    print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    return quality_report\n",
    "\n",
    "# Generate quality reports\n",
    "train_quality = create_data_quality_report(train_df, \"TRAINING DATA\")\n",
    "test_quality = create_data_quality_report(test_df, \"TESTING DATA\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 5: EXPLORATORY DATA ANALYSIS (EDA) - NUMERICAL FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "THEORY - Why analyze numerical features?\n",
    "=========================================\n",
    "\n",
    "Numerical features are continuous or discrete numbers (age, score, hours, etc.)\n",
    "\n",
    "We want to understand:\n",
    "1. DISTRIBUTION: How are values spread? (Normal? Skewed?)\n",
    "2. OUTLIERS: Are there extreme values that don't fit?\n",
    "3. RANGE: What's the minimum and maximum?\n",
    "4. CENTRAL TENDENCY: What's the average/median?\n",
    "\n",
    "VISUALIZATIONS WE'LL USE:\n",
    "- Histogram: Shows frequency distribution\n",
    "- Box Plot: Shows quartiles and outliers\n",
    "- KDE Plot: Smooth version of histogram\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nðŸ“ˆ ANALYZING NUMERICAL FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Identify numerical columns (excluding ID and target)\n",
    "numerical_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Remove ID column and target column if they exist\n",
    "if 'id' in numerical_cols:\n",
    "    numerical_cols.remove('id')\n",
    "if 'Id' in numerical_cols:\n",
    "    numerical_cols.remove('Id')\n",
    "\n",
    "# Identify target column (usually the last numerical column in train)\n",
    "# You'll need to adjust this based on your actual target column name\n",
    "target_col = 'score'  # Replace with your actual target column name\n",
    "\n",
    "if target_col in numerical_cols:\n",
    "    numerical_cols.remove(target_col)\n",
    "\n",
    "print(f\"Found {len(numerical_cols)} numerical features: {numerical_cols}\")\n",
    "print(f\"Target variable: {target_col}\")\n",
    "\n",
    "\n",
    "def plot_numerical_analysis(df, columns, target=None, ncols=2):\n",
    "    \"\"\"\n",
    "    Creates comprehensive visualizations for numerical columns\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The dataframe containing the data\n",
    "    columns : list\n",
    "        List of numerical column names to analyze\n",
    "    target : str, optional\n",
    "        Name of target column (for training data)\n",
    "    ncols : int\n",
    "        Number of columns in the subplot grid\n",
    "    \"\"\"\n",
    "    \n",
    "    if not columns:\n",
    "        print(\"No numerical columns to plot!\")\n",
    "        return\n",
    "    \n",
    "    nrows = (len(columns) + ncols - 1) // ncols  # Calculate rows needed\n",
    "    \n",
    "    # 1. Distribution Plots (Histogram + KDE)\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(15, 5*nrows))\n",
    "    axes = axes.flatten() if len(columns) > 1 else [axes]\n",
    "    \n",
    "    for idx, col in enumerate(columns):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Plot histogram with KDE\n",
    "        df[col].hist(bins=30, alpha=0.5, ax=ax, edgecolor='black')\n",
    "        df[col].plot(kind='kde', secondary_y=True, ax=ax, color='red', linewidth=2)\n",
    "        \n",
    "        ax.set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistics text\n",
    "        stats_text = f'Mean: {df[col].mean():.2f}\\nMedian: {df[col].median():.2f}\\nStd: {df[col].std():.2f}'\n",
    "        ax.text(0.7, 0.95, stats_text, transform=ax.transAxes, \n",
    "                fontsize=9, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(columns), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('numerical_distributions.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\nâœ“ Distribution plots saved as 'numerical_distributions.png'\")\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Box Plots (for outlier detection)\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(15, 5*nrows))\n",
    "    axes = axes.flatten() if len(columns) > 1 else [axes]\n",
    "    \n",
    "    for idx, col in enumerate(columns):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Create box plot\n",
    "        box_plot = ax.boxplot(df[col].dropna(), vert=True, patch_artist=True)\n",
    "        box_plot['boxes'][0].set_facecolor('lightblue')\n",
    "        box_plot['boxes'][0].set_alpha(0.7)\n",
    "        \n",
    "        ax.set_title(f'Box Plot of {col}', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel(col)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Calculate and display outlier count\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)][col]\n",
    "        \n",
    "        ax.text(0.5, 0.95, f'Outliers: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)', \n",
    "                transform=ax.transAxes, ha='center', va='top',\n",
    "                fontsize=9, bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(columns), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('numerical_boxplots.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"âœ“ Box plots saved as 'numerical_boxplots.png'\")\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Correlation with target (if target is provided)\n",
    "    if target and target in df.columns:\n",
    "        correlations = []\n",
    "        for col in columns:\n",
    "            corr = df[col].corr(df[target])\n",
    "            correlations.append({'Feature': col, 'Correlation': corr})\n",
    "        \n",
    "        corr_df = pd.DataFrame(correlations).sort_values('Correlation', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(10, max(6, len(columns)*0.4)))\n",
    "        colors = ['green' if x > 0 else 'red' for x in corr_df['Correlation']]\n",
    "        plt.barh(corr_df['Feature'], corr_df['Correlation'], color=colors, alpha=0.7)\n",
    "        plt.xlabel('Correlation with Target', fontsize=12)\n",
    "        plt.title(f'Feature Correlation with {target}', fontsize=14, fontweight='bold')\n",
    "        plt.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "        plt.grid(True, alpha=0.3, axis='x')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_correlations.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"âœ“ Correlation plot saved as 'feature_correlations.png'\")\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nCorrelation with Target:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(corr_df.to_string(index=False))\n",
    "\n",
    "# Run the analysis\n",
    "if numerical_cols:\n",
    "    plot_numerical_analysis(train_df, numerical_cols, target=target_col)\n",
    "else:\n",
    "    print(\"No numerical features found for analysis!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 6: EXPLORATORY DATA ANALYSIS (EDA) - CATEGORICAL FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "THEORY - Why analyze categorical features?\n",
    "===========================================\n",
    "\n",
    "Categorical features are non-numerical (gender, color, city, yes/no, etc.)\n",
    "\n",
    "We want to understand:\n",
    "1. CARDINALITY: How many unique categories?\n",
    "2. FREQUENCY: Which categories are most/least common?\n",
    "3. BALANCE: Are categories evenly distributed?\n",
    "4. RELATIONSHIP: How do categories relate to target?\n",
    "\n",
    "VISUALIZATIONS WE'LL USE:\n",
    "- Count Plot: Shows frequency of each category\n",
    "- Box Plot: Shows target distribution across categories\n",
    "- Percentage bars: Shows relative proportions\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nðŸ“Š ANALYZING CATEGORICAL FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = train_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Remove ID columns if present\n",
    "categorical_cols = [col for col in categorical_cols if 'id' not in col.lower()]\n",
    "\n",
    "print(f\"Found {len(categorical_cols)} categorical features: {categorical_cols}\")\n",
    "\n",
    "\n",
    "def plot_categorical_analysis(df, columns, target=None, ncols=2):\n",
    "    \"\"\"\n",
    "    Creates comprehensive visualizations for categorical columns\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The dataframe containing the data\n",
    "    columns : list\n",
    "        List of categorical column names to analyze\n",
    "    target : str, optional\n",
    "        Name of target column (for training data)\n",
    "    ncols : int\n",
    "        Number of columns in the subplot grid\n",
    "    \"\"\"\n",
    "    \n",
    "    if not columns:\n",
    "        print(\"No categorical columns to plot!\")\n",
    "        return\n",
    "    \n",
    "    nrows = (len(columns) + ncols - 1) // ncols\n",
    "    \n",
    "    # 1. Count Plots with Percentages\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(15, 5*nrows))\n",
    "    axes = axes.flatten() if len(columns) > 1 else [axes]\n",
    "    \n",
    "    for idx, col in enumerate(columns):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Calculate value counts and percentages\n",
    "        value_counts = df[col].value_counts()\n",
    "        percentages = df[col].value_counts(normalize=True) * 100\n",
    "        \n",
    "        # Create bar plot\n",
    "        value_counts.plot(kind='bar', ax=ax, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "        \n",
    "        ax.set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add percentage labels on bars\n",
    "        for i, (count, pct) in enumerate(zip(value_counts, percentages)):\n",
    "            ax.text(i, count, f'{pct:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        # Rotate x-labels if too many categories\n",
    "        if len(value_counts) > 5:\n",
    "            ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # Add statistics\n",
    "        stats_text = f'Unique: {df[col].nunique()}\\nMode: {df[col].mode()[0] if len(df[col].mode()) > 0 else \"N/A\"}'\n",
    "        ax.text(0.7, 0.95, stats_text, transform=ax.transAxes,\n",
    "                fontsize=9, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(columns), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('categorical_distributions.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\nâœ“ Categorical distribution plots saved as 'categorical_distributions.png'\")\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Relationship with Target (if target provided)\n",
    "    if target and target in df.columns:\n",
    "        fig, axes = plt.subplots(nrows, ncols, figsize=(15, 5*nrows))\n",
    "        axes = axes.flatten() if len(columns) > 1 else [axes]\n",
    "        \n",
    "        for idx, col in enumerate(columns):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            # Create box plot showing target distribution for each category\n",
    "            df.boxplot(column=target, by=col, ax=ax, patch_artist=True)\n",
    "            \n",
    "            ax.set_title(f'{target} by {col}', fontsize=12, fontweight='bold')\n",
    "            ax.set_xlabel(col)\n",
    "            ax.set_ylabel(target)\n",
    "            plt.sca(ax)\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            \n",
    "            # Remove the automatic title from boxplot\n",
    "            ax.get_figure().suptitle('')\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for idx in range(len(columns), len(axes)):\n",
    "            axes[idx].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('categorical_vs_target.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"âœ“ Categorical vs target plots saved as 'categorical_vs_target.png'\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Print detailed statistics\n",
    "        print(\"\\nCategorical Features Impact on Target:\")\n",
    "        print(\"-\" * 80)\n",
    "        for col in columns:\n",
    "            print(f\"\\n{col}:\")\n",
    "            stats = df.groupby(col)[target].agg(['mean', 'median', 'std', 'count'])\n",
    "            print(stats)\n",
    "\n",
    "# Run the analysis\n",
    "if categorical_cols:\n",
    "    plot_categorical_analysis(train_df, categorical_cols, target=target_col)\n",
    "else:\n",
    "    print(\"No categorical features found for analysis!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 7: CORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "THEORY - What is Correlation?\n",
    "==============================\n",
    "\n",
    "Correlation measures the relationship between two variables:\n",
    "- Value ranges from -1 to +1\n",
    "- +1: Perfect positive correlation (both increase together)\n",
    "- -1: Perfect negative correlation (one increases, other decreases)\n",
    "-  0: No correlation (no relationship)\n",
    "\n",
    "WHY IS THIS IMPORTANT?\n",
    "1. Helps identify which features are most related to target\n",
    "2. Detects multicollinearity (features too similar to each other)\n",
    "3. Guides feature selection\n",
    "\n",
    "CORRELATION HEATMAP:\n",
    "- Darker colors = stronger correlation\n",
    "- We look for features highly correlated with target\n",
    "- We avoid features highly correlated with each other (redundant)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nðŸ”— CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Select only numerical columns for correlation\n",
    "numerical_data = train_df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = numerical_data.corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # Mask upper triangle\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "            cmap='coolwarm', center=0, square=True, linewidths=1,\n",
    "            cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Heatmap of Numerical Features', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nâœ“ Correlation heatmap saved as 'correlation_heatmap.png'\")\n",
    "plt.show()\n",
    "\n",
    "# Find highly correlated feature pairs (excluding target)\n",
    "print(\"\\nHighly Correlated Feature Pairs (|correlation| > 0.7):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "            high_corr_pairs.append({\n",
    "                'Feature 1': correlation_matrix.columns[i],\n",
    "                'Feature 2': correlation_matrix.columns[j],\n",
    "                'Correlation': correlation_matrix.iloc[i, j]\n",
    "            })\n",
    "\n",
    "if high_corr_pairs:\n",
    "    high_corr_df = pd.DataFrame(high_corr_pairs).sort_values('Correlation', ascending=False)\n",
    "    print(high_corr_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No highly correlated feature pairs found!\")\n",
    "\n",
    "# Show correlation with target\n",
    "if target_col in correlation_matrix.columns:\n",
    "    print(f\"\\nCorrelation with Target ({target_col}):\")\n",
    "    print(\"-\" * 80)\n",
    "    target_corr = correlation_matrix[target_col].drop(target_col).sort_values(ascending=False)\n",
    "    print(target_corr)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY AND RECOMMENDATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ“‹ DATA ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. DATASET OVERVIEW:\")\n",
    "print(f\"   - Training samples: {len(train_df)}\")\n",
    "print(f\"   - Testing samples: {len(test_df)}\")\n",
    "print(f\"   - Total features: {len(train_df.columns) - 1}\")  # Excluding target\n",
    "print(f\"   - Numerical features: {len(numerical_cols)}\")\n",
    "print(f\"   - Categorical features: {len(categorical_cols)}\")\n",
    "\n",
    "print(\"\\n2. DATA QUALITY:\")\n",
    "missing_total = train_df.isnull().sum().sum()\n",
    "if missing_total > 0:\n",
    "    print(f\"   âš  Missing values detected: {missing_total} total\")\n",
    "    print(\"   â†’ Action needed: Handle missing values before modeling\")\n",
    "else:\n",
    "    print(\"   âœ“ No missing values detected\")\n",
    "\n",
    "print(\"\\n3. FEATURE INSIGHTS:\")\n",
    "if target_col in correlation_matrix.columns:\n",
    "    top_features = correlation_matrix[target_col].drop(target_col).abs().sort_values(ascending=False).head(5)\n",
    "    print(\"   Top 5 most correlated features with target:\")\n",
    "    for feat, corr in top_features.items():\n",
    "        print(f\"      - {feat}: {corr:.3f}\")\n",
    "\n",
    "print(\"\\n4. NEXT STEPS:\")\n",
    "print(\"   âœ“ Data exploration complete!\")\n",
    "print(\"   â†’ Ready for: Feature Engineering\")\n",
    "print(\"   â†’ Then: Model Training\")\n",
    "print(\"   â†’ Finally: Prediction & Submission\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nðŸŽ‰ PART 1 COMPLETE: DATA QUALITY REPORT & EDA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\"\"\"\n",
    "================================================================================\n",
    "WHAT WE'VE LEARNED SO FAR:\n",
    "================================================================================\n",
    "\n",
    "1. âœ“ How to load and inspect data\n",
    "2. âœ“ How to create a data quality report\n",
    "3. âœ“ How to analyze numerical features (distributions, outliers)\n",
    "4. âœ“ How to analyze categorical features (frequencies, relationships)\n",
    "5. âœ“ How to identify correlations between features\n",
    "6. âœ“ How to make decisions about which features to keep/modify\n",
    "\n",
    "NEXT TUTORIAL PARTS WILL COVER:\n",
    "- Part 2: Data Preprocessing & Feature Engineering\n",
    "- Part 3: Model Selection & Training\n",
    "- Part 4: Model Evaluation & Tuning\n",
    "- Part 5: Making Predictions & Submission\n",
    "\n",
    "================================================================================\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
